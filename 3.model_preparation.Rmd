---
title: "Model preparation"
output:
  # pdf_document:
  #   toc: true
  html_document:
  toc: true
---

Ctrl + Shift + Enter - run cell.
Ctrl + Alt + I - insert new cell.
Ctrl + Shift + K - preview HTML result document.

# Packages
```{r}
library(leaps)
```


# 1. Download data

This is the third part of the project. After cleaning data, exploring and 
engineering features, we prepare linear, non linear and generalized linear
models.

```{r}
file_path = './Data/OnlineNewsPopularity_processed.csv'
data = read.csv(file_path)
print(dim(data))
```

```{r}
data = data[, -1] # removing url column
stopifnot(dim(data)[2] == 60)
```


We randomly subset 3000 training points out of the initial dataset. This is caused
by R requirements for storing model parameters (algorithms refused to run on
15,000 instances).

```{r}
subset_size = 3000  # Number of points that we use from the dataset

subsample_indices = sample.int(
  n = nrow(data),
  size = subset_size
)

data = data[subsample_indices, ]
stopifnot(dim(data)[1] == subset_size)
```

```{r}
train_percent = 0.6
val_percent = 0.2
train_val_percent = train_percent + val_percent
test_percent = 1 - train_val_percent
```

We randomly split our subset into train, validation and test sets.

```{r}
train_val_sample = sample.int(
  n = nrow(data),
  size = floor(train_val_percent * nrow(data))
)

train_val = data[train_val_sample, ]
test = data[-train_val_sample, ]

train_sample = sample.int(
  n = nrow(train_val),
  size = floor(train_percent * nrow(data)) 
)

train = train_val[train_sample, ]
validation = train_val[-train_sample, ]

ncat = function(...){
  cat(..., '\n')
}

ncat('Train size', nrow(train))
ncat('Validation size', nrow(validation))
ncat('Test size', nrow(test))
```

```{r}
Y_colname = colnames(data)[length(colnames(data))]
stopifnot(Y_colname == 'shares')

X_colnames = colnames(data)[1 : length(colnames(data)) - 1]

# X = as.matrix(data[, X_colnames])
# Y = data[, Y_colname]

stopifnot(length(Y_colname) + length(X_colnames) == dim(data)[2])
stopifnot(dim(train)[2] - 1 == dim(train[, X_colnames])[2])
```

# Linear regression assumptions

To use linear models, we have to answer a question - are they appropriate
for this task? To do that, we will check the compliance with linear model 
assumptions:

1. There is a linear relationship between the predictors (x) and the outcome (y)
2. Predictors (x) are independent and observed with negligible error
3. Residual Errors have a mean value of zero
4. Residual Errors have constant variance
5. Residual Errors are independent from each other and predictors (x)

```{r}
assumption_test_model = lm(train[,Y_colname] ~ ., data = train[, X_colnames])
assumption_test_model_log = lm(log(train[,Y_colname]) ~ ., data = train[, X_colnames])

assumption_test_model_sqrt = lm(
  sqrt(train[,Y_colname]) ~ ., 
  data = train[, X_colnames]
)
```



### Assumption 1: Check linearity of the data

To check, if there is a linear relationship between target and predictors (linearity of the data), we 
will use Residuals VS Fitted plot

From the plots below, we see that residual plots for
GLM model with log(Y) transform has residuals, centered
around zero, without funnel shape, while full linear model demonstrates the decline of residuals. 
This speaks in favor of appropriateness of log(Y) transform
and using generalized linear model (please, pay attention to the scale of the
Y-axis, this is where the difference is clear).


```{r}
par(mfrow = c(1, 1))

plot(assumption_test_model, 1)
plot(assumption_test_model_log, 1)
plot(assumption_test_model_sqrt, 1)

par(mfrow = c(1, 1))
```

### Assumption 2: Independence of predictors

Second assumption - is that predictors are independent
and observed with negligible error.

For this, we use Durbin Watson test. The null hypothesis of the test states that there is no 
auto-correlation of residuals. Implicitly, our
target is not enough evidence for rejecting H0 hypotheses. 

We perform the test for full linear and GLM models.
As seen below, both statistics give evidence in 
favor of correlated residuals, which is an argument
against using GLM and linear models.


```{r}
library(car)
durbinWatsonTest(assumption_test_model)
durbinWatsonTest(assumption_test_model_log)
durbinWatsonTest(assumption_test_model_sqrt)
```

### Assumption 3: residual errors have zero mean value

From the residual plots below, we can see that 
GLM model has almost zero mean of residuals.

```{r}
par(mfrow = c(2, 2))

plot(assumption_test_model, 1)
plot(assumption_test_model_log, 1)
plot(assumption_test_model_sqrt, 1)

par(mfrow = c(1, 1))
```

### Assumption 4: residual errors have constant variance

Unfortunately, this assumption is not met for both models. The scale-location
plot is assumed to be centered around zero.

```{r}
par(mfrow = c(2, 2))

plot(assumption_test_model, 3)
plot(assumption_test_model_log, 3)
plot(assumption_test_model_sqrt, 3)

par(mfrow = c(1, 1))
```
Also, we can use an NCV (non constant error variance)
test for these models. As it is seen from the cell 
below, this assumption is not met.

```{r}
library(car)

ncvTest(assumption_test_model)
ncvTest(assumption_test_model_log)
ncvTest(assumption_test_model_sqrt)
```
As a conclusion, usage of linear models and GLM is debatable for this task. However, 
for the log(Y) transformation, this model meets several assumptions. So, we 
will consider this model and compare it with the rest.

In the cells above, let us train several linear models. We leverage a range of 
linear models, with and without parameters selection and regularization techniques.
We experiment with following models:
- Full linear model
- Poisson GLM (log transform of target variable)
- Backward and forward coefficients selection
- Best model, based on Bayesian Information Criterion
- Best model, based on Mallow's Cp coefficient
- Ridge regression model
- Lasso regression model (to enforce sparsity)

After training the models, we compare and select the best in terms of:
- Adjuster R squared
- Akaike Information Criterion
- Bayesian Information Criterion
- Number of paramters
- Validation R squared

Moreover, we share our considerations, about which criteria should be prioritized
for model selection

# 1. Full linear model

```{r}
full_model = lm(train[,Y_colname] ~ ., data = train[, X_colnames])
```

## Model summary

The adjusted R squared for full linear model is only 0.12. With the following
techniques, we will try to improve this metrics.

```{r}
summary(full_model)
```

```{r}
par(mfrow = c(2, 2))
plot(full_model)
par(mfrow = c(1, 1))
```

### Model residuals plot

```{r}
plot_residuals <- function(model){
  plot(
    fitted(model),
    residuals(model),
    col = 'gray40',
    xlab = 'fitted values',
    ylab = 'residuals'
  )
  
  lines(
    loess.smooth(fitted(model), residuals(model)),
    col = "blue",
    lwd = 2
  )
  abline(h = 0, lty = 3)
}
```

```{r}
plot_residuals(full_model)
```

# Full linear model (Y log transform) -- Poisson GLM

```{r}
full_model_log = lm(log(train[,Y_colname]) ~ ., data = train[, X_colnames])
```

## Model summary

```{r}
qqnorm(rstandard(full_model_log))
qqline(rstandard(full_model_log))
```
Log transformation helped to increase R squared up to 0.15

```{r}
summary(full_model_log)
```


```{r}
par(mfrow = c(2, 2))
plot(full_model_log)
par(mfrow = c(1, 1))
```

Also, we note that the residuals are centered closer around zero, compared to 
full linear model.

```{r}
plot_residuals(full_model_log)
```


# Linear model (forward / backward model selection)

However, we can reduce variance of the model with backward and forward model 
selection. Empirically, backward selection works better. However, we also
keep the forward selection for comparison.

```{r}
correlated_indices = c(
  match('weekday_is_friday', names(train)),
  match('weekday_is_saturday', names(train))
)
# match
stopifnot(sum(is.na(correlated_indices)) == 0)


nvmax = 20

forward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

backward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

## Model summary

```{r}
plot_max_point <- function(values){
  max_idx = which.max(values)
  points(max_idx, values[max_idx], col = 'red', cex = 2, pch = 20)
}

plot_min_point <- function(values){
  min_idx = which.min(values)
  points(min_idx, values[min_idx], col = 'red', cex = 2, pch = 20)
}

plot_subsets_summary <- function(subsets_model){
  subset_summary = summary(subsets_model)
  
  xlabel = 'Number of iterations'
  linetype = 'l'
  
  par(mfrow = c(2, 2))
  
  plot(subset_summary$rss, xlab = xlabel, ylab = 'RSS', type = linetype)
  plot_min_point(values = subset_summary$rss)
  
  plot(subset_summary$adjr2, xlab = xlabel, ylab = 'Adjusted R2', type = linetype)
  plot_max_point(values = subset_summary$adjr2)
  
  plot(subset_summary$cp, xlab = xlabel, ylab = 'Cp', type = linetype)
  plot_min_point(values = subset_summary$cp)
  
  plot(subset_summary$bic, xlab = xlabel, ylab = 'BIC', type = 'l')
  plot_min_point(values = subset_summary$bic)
  
  
  # max_idx = which.max(subset_summary$adjr2)
  # points(max_idx, reg.summary$adjr2[max_idx], col="red",cex=2,pch=20)
  
  par(mfrow = c(1, 1))
}
```

```{r}
plot_subsets_summary(forward_subsets)
```

```{r}
plot_subsets_summary(backward_subsets)
```
In the code cells below, we aim to select best models after forward and backward 
elimination - in terms of BIC and Mallow's Cp coefficients.

```{r}
get_best_n_params <- function(subsets_model){
  model_summary = summary(subsets_model)
  best_bic = which.min(model_summary$bic)
  best_cp = which.min(model_summary$cp)
  return (list('best_bic' = best_bic, 'best_cp' = best_cp))
}

get_best_coefficients <- function(subsets_model){
  best_params = get_best_n_params(subsets_model)
  
  n_best_bic = best_params$best_bic
  n_best_cp = best_params$best_cp
  
  best_bic_coefs = names(coefficients(subsets_model, n_best_bic))[-1]
  best_cp_coefs = names(coefficients(subsets_model, n_best_cp))[-1]
  
  return(list("best_bic_coefs" = best_bic_coefs, "best_cp_coefs" = best_cp_coefs))
}

best_params_back = get_best_n_params(backward_subsets)
best_params_forward = get_best_n_params(forward_subsets)

n_best_bic_back = best_params_back$best_bic
n_best_cp_back = best_params_back$best_cp

n_best_bic_fwd = best_params_forward$best_bic
n_best_cp_fwd = best_params_forward$best_cp

```


```{r}
cat('Best # of parameters (backward)', n_best_bic_back, n_best_cp_back, '\n')
cat('Best # of parameters (forward)', n_best_bic_fwd, n_best_cp_fwd, '\n')
```

```{r}
best_forward_coefs = get_best_coefficients(forward_subsets)
best_backward_coefs = get_best_coefficients(backward_subsets)

bic_forward_coefs = best_forward_coefs$best_bic_coefs
bic_backward_coefs = best_backward_coefs$best_bic_coefs

cp_forward_coefs = best_forward_coefs$best_cp_coefs
cp_backward_coefs = best_backward_coefs$best_cp_coefs
```

Here are the chosen coefficients after model selection:

```{r}
print(cp_backward_coefs)
print(bic_backward_coefs)
```
To represent the most significant variables, here we report the variable elimination
plots:

```{r}
plot(backward_subsets, scale = 'r2')
```

```{r}
plot(forward_subsets, scale = 'r2')
```

After elimination of variables, let us retrain best BIC and Cp model:


```{r}
bic_best_model = lm(train[, Y_colname] ~ ., data = train[, bic_backward_coefs])
```

```{r}
cp_best_model = lm(train[, Y_colname] ~ ., data = train[, cp_backward_coefs])
```


# Forward / backward selection for log(Y) transform

Here we reiterate the same procedure, taking Log() transformation of target

```{r}

nvmax = 20

log_forward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

log_backward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

```{r}
plot_subsets_summary(log_forward_subsets)
```

```{r}
plot_subsets_summary(log_backward_subsets)
```


```{r}
log_best_coefficients = get_best_coefficients(log_backward_subsets)
log_bic_backward_coefs = log_best_coefficients$best_bic_coefs
log_cp_backward_coefs = log_best_coefficients$best_cp_coefs
```


```{r}
cat(
  'Number of best coefficients', 
  length(log_bic_backward_coefs), 
  length(log_cp_backward_coefs),
  '\n'
)
log_bic_backward_coefs
log_cp_backward_coefs
```


```{r}
log_cp_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_cp_backward_coefs])
```

```{r}
log_bic_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_bic_backward_coefs])
```


# Ridge regression

For the Ridge regression, it is necessary to standardize data:

```{r}
standardize_data <- function(df, target){
  vals = df[, target]
  stand_data = as.data.frame(scale(df))
  stand_data[, target] = vals
  return(stand_data)
}
```

```{r}
standardized_train = standardize_data(train, 'shares')
standardized_validation = standardize_data(validation, 'shares')
standardized_test = standardize_data(test, 'shares')
```

```{r}
prepare_ridge_lasso <- function(X, Y, alpha){
  grid <- 10^seq(10, -2, length=100)
  
  library(glmnet)
  
  model <- glmnet(
    X, 
    Y,
    alpha = alpha,  # 0 for Ridge Regression
    lambda = grid,
    standardize = F
  )
  
  

  set.seed(1)
  cv.out <- cv.glmnet(
    X,
    Y,
    alpha = alpha,
    nfold=10
  )
  
  plot(cv.out)
  
  i.bestlam <- which.min(cv.out$cvm)
  bestlam <- cv.out$lambda[i.bestlam]
  cat('Best lambda:', bestlam, '\n')
  
  return(list('model' = model, 'lambda' = bestlam))
}
```


```{r}
X = as.matrix(standardized_train[, -length(colnames(standardized_train))])
Y = as.matrix(standardized_train$shares)

ridge_res = prepare_ridge_lasso(
  X = X,
  Y = Y,
  alpha = 0  # Ridge
)

ridge_model = ridge_res$model
ridge_lamda = ridge_res$lambda
```


```{r}
ridge_predictions <- predict(
  ridge_model,
  s = ridge_lamda,
  newx = as.matrix(
    standardized_validation[, -length(colnames(standardized_validation))]
  )
)

Ridge_MSE_val = mean((ridge_predictions - standardized_validation$shares)^2)
```


# Lasso Regression

Thanks to L1 norm, lasso regression model enforces sparsity of coefficients. Let
us try to build this model as well.

```{r}
lasso_res = prepare_ridge_lasso(
  X = X,
  Y = Y,
  alpha = 1  # Lasso
)

lasso_model = lasso_res$model
lasso_lambda = lasso_res$lambda
```
```{r}
lasso_predictions <- predict(
  lasso_model, 
  s = lasso_lambda,
  newx = as.matrix(
    standardized_validation[, -length(colnames(standardized_validation))]
  )
)

lasso_MSE_val = mean((lasso_predictions - standardized_validation$shares)^2)
```

# Comparison of models

How do we choose proper criteria for comparing models? We have several options:
1. R^2 (adjusted) on validation set
2. R^2 (adjusted) on train set
3. Bayesian Information Criteria
4. Akaike information criteria
5. Mallow's Cp criteria

We select the proper characteristics, according to following considerations:
1. Cp, AIC, BIC and adjusted R^2 - all these account for both good fit
and simplicity of the model
2. Validation R^2 parameter is preferred, as it shows the performance on 
the unseen data. Ideally, a cross-validation adjusted R^2 metrics will
give better understanding
3. Mallow's Cp and AIC are equivalent and result in the selection of
the same model
4. We are looking to minimize Cp, AIC and BIC, but to maximize R^2-related
metrics
5. BIC statistics penalizer big models heavier that Cp and AIC --> BIC criterion
results in selection of more 'lightweight' model

According to this, we will compare the models based on following characteristics:
1. (Cross) validation adjusted R^2
2. AIC (or, equivalently, Mallow's Cp)
3. BIC (BIC criterion will be of the highest priority, if we target at the
simplest model possible)


```{r}
models = list(
  full_model = list(name = 'full', model = full_model),
  full_model_log = list(name = 'full log', model = full_model_log),
  
  backward_bic_best_model = list(name = 'bck. bic', model = bic_best_model),
  backward_cp_best_model = list(name = 'bck. cp', model = cp_best_model),
  
  backward_cp_best_model_log = list(name = 'bck. cp log', 
                                    model = log_cp_best_model),
  backward_bic_best_model_log = list(name = 'bck. bic log',
                                     model = log_bic_best_model)
)
```

## Comparison of adjusted R^2

Main conclusion that can be made here - is that generalized linear model was capable
of achieving better R squared results, independently of parameter selection technique.

```{r}

plot_ordered_hist <- function(X, Y, main, xlab, ylab){
  data = data.frame(Y, X)
  barplot(
    data[order(data[,1],decreasing=TRUE),][,1],
    names.arg=data[order(data[,1],decreasing=TRUE),][,2],
    main = main,
    xlab = xlab,
    ylab = ylab
  )
}

names = c()
r.squareds = c()
for(m in models){
  names = append(names, m$name)  
  r.squareds = append(r.squareds, summary(m$model)$adj.r.squared)
}

plot_ordered_hist(
  X = names,
  Y = r.squareds,
  main = 'Adjusted R sqared',
  xlab = 'Model name',
  ylab = 'Adjusted R squared'
)
```
## Comparison of AIC

A remarkable 90% difference is also observed in terms of Akaike and Bayesian 
information criteria:

```{r}
library(olsrr)

AICs = c()

for(m in models){
  AICs = append(AICs, AIC(m$model))
}

AICs_norm = AICs / max(AICs)
plot_ordered_hist(
  X = names, 
  Y = AICs_norm, 
  main = 'Normalized Akaike Information Criterion (AIC)', 
  xlab = 'Model name', 
  ylab = 'AIC'
)
```
## Comparison of BIC

```{r}
BICs = c()

for(m in models){
  BICs = append(BICs, BIC(m$model))
}

BICs_norm = BICs / max(BICs)
plot_ordered_hist(
  X = names, 
  Y = BICs_norm, 
  main = 'Normalized Bayesian Information Criterion (BIC)', 
  xlab = 'Model name', 
  ylab = 'BIC'
)
```

## Comparison of parameters number

A powerful result can be made here - having up to 70 percent less parameters,
the lightweight models still were able to achieve better R squared metrics.

```{r}
model_sizes = c()
for(m in models){
  model_sizes = append(model_sizes, length(coefficients(m$model)))
}
model_sizes_norm = model_sizes / max(model_sizes)

plot_ordered_hist(
  X = names, 
  Y = model_sizes_norm, 
  main = 'Number of coefficients', 
  xlab = 'Model name', 
  ylab = 'Normalized number of coefficients'
)
```

## Comparison on the validation set

On the validation set, all models achieved comparable results. Lasso and Ridge
regression reached the least R squared metrics. However, currently we miss comparison
with these models based on BIC, Cp and AIK criteria.

```{r}
library(stringr)

validation_r2 = c()

for(m in models){
  model = m$model
  name = m$name

  preds = predict(
    model,
    data = validation[, coefficients(model)]
  )
  
  if(str_detect(name, 'log')){
    preds = exp(preds)
  }
  
  r2 = mean((preds - test$shares)^2)
  cat('Model:', name, 'MSE validation:', r2, '\n')
  validation_r2 = append(validation_r2, r2)
}


names_ = append(names, c('Ridge', 'Lasso'))
validation_r2_ = append(validation_r2, c(1.0, 1.0))

plot_ordered_hist(
  X = append(names, c('Ridge', 'Lasso')), #_names,
  Y = append(validation_r2, c(Ridge_MSE_val, lasso_MSE_val)), #_validation_r2,
  main = 'MSE validation set', 
  xlab = 'Model name', 
  ylab = 'MSE'
)
```

Our final conclusion regarding the linear models:
The best model is a GLM, with backward coefficients selection. It has
70% less parameters than full model, allowing for aroung 0.15 adjusted R squared.
However, not all the assumptions of using linear models are met.




