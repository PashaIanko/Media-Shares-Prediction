---
title: "Data processing"
# output: html_notebook
output:
  # pdf_document:
  #   toc: true
  html_document:
  toc: true
---


This file is dedicated to the preprocessing of the dataset, with saving the processed
data for training and comparing the models.



# 1. Obtaining data

The dataset represents a metadata of the articles, published by a web resource <https://mashable.com/>, up to January 8, 2015. A csv file was downloaded from the Machine Learning Repository, created by University of California, USA (<https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity>).


```{r}
file_path = './Data/OnlineNewsPopularity.csv'
save_path = './Data/OnlineNewsPopularity_processed.csv'
data = read.csv(file_path)
```


# 2. Clean and filter data

First thing to notice is the large data size. To handle almost 40000 data instances, the articles were later split into train, validation and test sets.

```{r}
# Shape
print('Data frame shape:')
dim(data)
```



## 2.1. Data types

The dataset consists entirely of numerical data, except for the first column, indicating an url reference to the article. 

```{r}
print('Unique data types:')
print(unique(sapply(data, class)))
```
We decide to drop the url column, as it can be used only for the analysis of the article's text (e.g., with Recurrent Neural Network), which is beyond the scope of this project.

```{r}
library(purrr)

num_cols = purrr::map_lgl(data, is.numeric)
num_cols = names(num_cols[num_cols == TRUE])
# head(data[, num_cols])

stopifnot(dim(data[, num_cols]) == c(39644, 60))
```

## 2.2. Not-a-number, Infinity, factor variables check

In the two chunks below, we make sure the dataset does not contain NAN and infinity values, as well as factor variables, requiring special treatment.

```{r}
get_nan_cols <- function(df) {
  # Check NA and NAN values
  
  cols = colnames(df)
  res = c()
  for (i in cols){
    contains_nan = length(df$i[is.nan(df$i)])
    
    if (contains_nan == TRUE){
      res = append(res, i)
    }
  }
  return(res)
}

get_inf_cols <- function(df){
  res = c()
  cols = colnames(df)
  
  for (i in cols){
    contains_inf = TRUE %in% is.infinite(df$i)
    if (contains_inf == TRUE){
      res = append(res, col)
    }
  }
  return(res)
}
```


```{r}
nan_cols = get_nan_cols(data)
inf_cols = get_inf_cols(data)

stopifnot(length(nan_cols) == 0)
stopifnot(length(inf_cols) == 0)

# Check that we dont have factor variables
for(c in colnames(data)){
  stopifnot(!is.factor(data[, c]))
}
```



## 2.3. Skewness & Distribution Check

Before cleaning the data, we consider important looking at the distribution
of the variables. Thus, the cells below contain necessary functions for plotting histograms with all numerical variables in the dataset.

```{r}
plot_hists <- function(data, features, n_cols){
  
  n_features = length(features)
  n_rows = as.integer(n_features / n_cols)
  # n_cols = as.integer(n_features / n_rows)

  while(n_cols * n_rows < n_features){
    n_rows = n_rows + 1
  }

  # Check if enough grid space for all plots
  stopifnot(as.integer(n_cols * n_rows) >= n_features)  
  par(mfrow = c(as.integer(n_rows), as.integer(n_cols)))

  par(mfrow = c(as.integer(n_rows), as.integer(n_cols)))
  for (colname in features){
    if(!is.na(colname)){
      hist(data[, colname], main = colname)
    }
  }
  
  par(mfrow = c(1, 1))
}

plot_all_hists <- function(data, cols){
  index_from = 1
  portion_len = 6
  
  
  to_plot = TRUE
  while(to_plot){
    
    index_to = min(index_from + portion_len - 1, length(num_cols))
    if(index_to == length(num_cols)){
      to_plot = FALSE
    }
    
    plot_hists(
      data,
      features = cols[index_from : index_to],
      n_cols = 3
    )
    
    index_from = index_from + portion_len
  
  }
}
```

From the cell below, several important observations were made:

1. Bimodality of variables
    - This insight will be used later for performing statistical tests and feature engineering. 
    For example, one can see Bernoulli distribution for such columns, regarding:
      - Topic of the publication:
        - data_channel_is_lifestyle
        - data_channel_is_entertainment
        - data_channel_is_socmed (social media)
        - data_channel_is_tech (technology)
        - data_channel_is_world
      - Day of the publication:
        - weekday_is_monday
        - weekday_is_tuesday
        - weekday_is_wednesday
        - weekday_is_thursday
        - weekday_is_friday
        - weekday_is_saturday
        - weekday_is_sunday

2. Visually, high skewness of some features
    - num_hrefs (number of hyperlinks in the publication)
    - num_imgs (number of images - features that correspond to counting, are 
    often found skewed)
    - global_rate_positive_words (rate of positive words, taking into account
    neutral tokens)
    
3. Varied ranges of the features and necessity of scaling
  - While some features (for example, day of the week) are ranged from 0 to 1,
  others (e.g. n_tokens_content) have maximum of 8474.
  

4. Potential outliers and necessity of IQR filtering
  - For example, mean of n_non_stop_words is 0.99, while maximum corresponds to 
  1042. There is a need to understand the nature of outliers, or remove them

```{r}
plot_all_hists(data, num_cols)
```

## 2.4. Apply cube root transformation to skewed columns

In this part, we normalize the most highly skewed columns with cubical root 
transformation

```{r}
check_skewness <- function(data, columns, skew_thresh, verbose = F){
  library(moments)
  res = c()
  for(column in columns){
    
    sk = skewness(data[, column])

    if (!(is.nan(sk)) && abs(sk) >= skew_thresh){
      
      if(verbose == T){
       cat('Column', column, 'skew =', sk, '\n')
      }
      res = append(res, column)
    }
  }
  return(res)
}


# skewness(data$max_negative_polarity)
skewed_cols = check_skewness(
  data, 
  num_cols,
  skew_thresh = 3,
  verbose = F
)
print(length(skewed_cols))
cat('skewed cols:', skewed_cols, '\n')
```

```{r}
cat('We have', length(skewed_cols), 'skewed columns', '\n')
plot_all_hists(data, skewed_cols)
```

```{r}
skewness_rootfix <- function(data, columns){
  library(moments)
  
  for(column in columns){
    # print(column)
    is_na_before = sum(is.na(data[, column]))
    data[, column] = (data[, column])^(1 / 3)
    is_na_after = sum(is.na(data[, column]))
    
    if(is_na_before == 0 && is_na_after > 0){
      cat('Problems with nans in column', column, '\n')
    }
  }
  return(data)
}


data = skewness_rootfix(
  data, 
  skewed_cols[
    -c(
      which(skewed_cols == 'kw_min_min'),
      which(skewed_cols == 'kw_avg_min'),
      which(skewed_cols == 'max_negative_polarity')
    )
  ]
)
```


```{r}
plot_all_hists(data, skewed_cols)
stopifnot(sum(is.na(data)) == 0)
```


## 2.5. Outlier analysis

As we made sure we have candidates for outliers, in the cells below we calculate
the outliers for all numerical columns. We consider outliers the values that
are beyond the range of 3 inter quartile range. 

One of the parameters in our 
function `iqr_statistics` is `threshold_perc`, set to 0.1. This means, that the
below cells report the columns, which contain more than 10% of outliers.

```{r}
calc_outliers_percent <- function(data, col, q_low, q_high){
  col_data = data[, col]
  
  Q1 = quantile(col_data, q_low, names = FALSE)
  Q2 = quantile(col_data, q_high, names = FALSE)
  IQR = Q2 - Q1
  
  
  no_outliers <- subset(
      data, (data[, col] > (Q1 - 1.5*IQR)) & (data[, col] < (Q2 + 1.5*IQR))
  )
  
    
  outliers_percent = (dim(data)[1] - dim(no_outliers)[1]) / dim(data)[1]
  #cat('calc', outliers_percent, '\n')
  
  return (list('Q1' = Q1, 'Q2' = Q2, 'IQR' = Q2 - Q1, 'percent' = outliers_percent))
  
}

iqr_statistics <- function(data, columns, q_low, q_high, threshold_perc, verbose){
  
  # return columns with > perc_thresh percent of outliers
  
  outlier_cols = c()
  
  for(col in columns){
    # print(col)
    col_data = data[, col]
    
    
    res = calc_outliers_percent(data, col, q_low, q_high)
    Q1 = res$Q1
    Q2 = res$Q2
    IQR = res$IQR
    
    no_outliers <- subset(
      data, (data[, col] > (Q1 - 1.5*IQR)) & (data[, col] < (Q2 + 1.5*IQR))
    )
    # print(dim(no_outliers))
    # outliers_percent = (dim(data)[1] - dim(no_outliers)[1]) / dim(data)[1]
    
    outliers_percent = res$percent
    
    if (outliers_percent > threshold_perc){
      if(verbose == T){
        cat('Column', col, ': % of outliers:', outliers_percent, '\n')
      }
      
      outlier_cols = append(outlier_cols, col)
    }
    
    
    
  }
  return(outlier_cols)
}


outlier_cols = iqr_statistics(
  data, 
  num_cols,
  q_low = 0.25,
  q_high = 0.75,
  threshold_perc = 0.1,
  verbose = T
)
```

Now let us have a closer look at the histograms for columns with outliers. 
We see a lot of Binomially distributed variables, and keep them for statistical
tests. However, several columns are candidates for IQR filtering:

- LDA_00
- LDA_01
- title_sentiment_polarity
- self_reference_min_shares


```{r}
plot_all_hists(data, outlier_cols)
```

Based on that, we choose some columns to perform IQR filtering. Here, we also 
see a lot of features with 'two-populations' distribution

```{r}
columns_to_iqr_clean = c(
  'num_self_hrefs',
  'title_sentiment_polarity',
  'LDA_00',
  'LDA_01',
  'kw_max_min',
  'self_reference_min_shares'
)

plot_all_hists(data, columns_to_iqr_clean)
```

```{r}
iqr_clean <- function(data, columns, q_low, q_high){
  
  # First, we need to collect the data on q1 and q3 values
  colnames = c()
  q1s = c()
  q3s = c()
  percents = c()
  
  for (i in 1:length(columns)){
    colname = columns[i]
    col_data = data[, colname]
    res = calc_outliers_percent(data, colname, q_low, q_high)
    
    colnames = append(colnames, colname)
    q1s = append(q1s, res$Q1)
    q3s = append(q3s, res$Q2)
    percents = append(percents, res$percent)
  }

  for (i in 1:length(colnames)){
    colname = colnames[i]
    q1 = q1s[i]
    q3 = q3s[i]
    outliers_percent = percents[i]
    print(q1, q3)
    iqr = q3 - q1

    cat('Col:', colname, 'Outliers %', outliers_percent, '\n')

    col_data = data[, colname]
    data = subset(
        data,
        (col_data > (q1 - 1.5 * iqr)) & (col_data < (q3 + 1.5 * iqr))
    )
    cat('New data size:', dim(data), '\n')
  }
  return(data)
  
}


data = iqr_clean(
  data, 
  columns = columns_to_iqr_clean,
  q_low = 0.25,
  q_high = 0.75
)
```

```{r}
plot_all_hists(data, columns_to_iqr_clean)
```



# 3. Saving data

```{r}
dim(data)
```

```{r}
write.csv(data, save_path, row.names = F)
```












