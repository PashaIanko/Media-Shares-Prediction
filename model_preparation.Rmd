---
title: "Data processing"
output: html_notebook
---

Ctrl + Shift + Enter - run cell.
Ctrl + Alt + I - insert new cell.
Ctrl + Shift + K - preview HTML result document.

# Packages
```{r}
library(leaps)
```


# Download data

```{r}
file_path = './Data/OnlineNewsPopularity_processed.csv'
data = read.csv(file_path)
print(dim(data))
head(data)
```

```{r}
data = data[, -1] # removing url column
head(data)
stopifnot(dim(data)[2] == 60)
```


# Subsetting training data

```{r}
subset_size = 3000  # Number of points that we use from the dataset

subsample_indices = sample.int(
  n = nrow(data),
  size = subset_size
)

data = data[subsample_indices, ]
stopifnot(dim(data)[1] == subset_size)
```

```{r}
train_percent = 0.6
val_percent = 0.2
train_val_percent = train_percent + val_percent
test_percent = 1 - train_val_percent
```

```{r}
train_val_sample = sample.int(
  n = nrow(data),
  size = floor(train_val_percent * nrow(data))
)

train_val = data[train_val_sample, ]
test = data[-train_val_sample, ]

train_sample = sample.int(
  n = nrow(train_val),
  size = floor(train_percent * nrow(data)) 
)

train = train_val[train_sample, ]
validation = train_val[-train_sample, ]

ncat = function(...){
  cat(..., '\n')
}

ncat('Train size', nrow(train))
ncat('Validation size', nrow(validation))
ncat('Test size', nrow(test))
```

```{r}
Y_colname = colnames(data)[length(colnames(data))]
stopifnot(Y_colname == 'shares')

X_colnames = colnames(data)[1 : length(colnames(data)) - 1]

# X = as.matrix(data[, X_colnames])
# Y = data[, Y_colname]

stopifnot(length(Y_colname) + length(X_colnames) == dim(data)[2])
stopifnot(dim(train)[2] - 1 == dim(train[, X_colnames])[2])
```

# Linear regression assumptions

To use linear models, we have to answer a question - are they appropriate
for this task? To do that, we will check the compliance with linear model 
assumptions:

1. There is a linear relationship between the predictors (x) and the outcome (y)
2. Predictors (x) are independent and observed with negligible error
3. Residual Errors have a mean value of zero
4. Residual Errors have constant variance
5. Residual Errors are independent from each other and predictors (x)

```{r}
assumption_test_model = lm(train[,Y_colname] ~ ., data = train[, X_colnames])
assumption_test_model_log = lm(log(train[,Y_colname]) ~ ., data = train[, X_colnames])

assumption_test_model_sqrt = lm(
  sqrt(train[,Y_colname]) ~ ., 
  data = train[, X_colnames]
)
```



### Assumption 1: Check linearity of the data

To check, if there is a linear relationship between target and predictors (linearity of the data), we 
will use Residuals VS Fitted plot

From the plots below, we see that residual plots for
GLM model with log(Y) transform has residuals, centered
around zero, without funnel shape, while full linear model demonstrates the decline of residuals. This speaks in favor of appropriateness of log(Y) transform
and using linear model.


```{r}
par(mfrow = c(1, 1))

plot(assumption_test_model, 1)
plot(assumption_test_model_log, 1)
plot(assumption_test_model_sqrt, 1)

par(mfrow = c(1, 1))
```

### Assumption 2: Independence of predictors

Second assumption - is that predictors are independent
and observed with negligible error.

For this, we use Durbin Watson test. The null hypothesis of the test states that there is no 
auto-correlation of residuals. Implicitly, our
target is not enough evidence for rejecting H0 hypotheses. 

We perform the test for full linear and GLM models.
As seen below, both statistics give evidence in 
favor of correlated residuals, which is an argument
against using linear model.

However, for the log(Y) model, the p-value level
almost achieves 0.05, without feature selection. Therefore, we still can try to take advantage of this
model.

```{r}
library(car)
durbinWatsonTest(assumption_test_model)
durbinWatsonTest(assumption_test_model_log)
durbinWatsonTest(assumption_test_model_sqrt)
```

### Assumption 3: residual errors have zero mean value

From the residual plots below, we can see that 
GLM model has almost zero mean of residuals.

```{r}
par(mfrow = c(2, 2))

plot(assumption_test_model, 1)
plot(assumption_test_model_log, 1)
plot(assumption_test_model_sqrt, 1)

par(mfrow = c(1, 1))
```

### Assumption 4: residual errors have constant variance

Unfortunately, this assumption is not met for both models. The scale-location
plot is assumed to be centered around zero.

```{r}
par(mfrow = c(2, 2))

plot(assumption_test_model, 3)
plot(assumption_test_model_log, 3)
plot(assumption_test_model_sqrt, 3)

par(mfrow = c(1, 1))
```
Also, we can use an NCV (non constant error variance)
test for these models. As it is seen from the cell 
below, this assumption is not met.

```{r}
library(car)

ncvTest(assumption_test_model)
ncvTest(assumption_test_model_log)
ncvTest(assumption_test_model_sqrt)
```
### Assumption 5: Residual Errors Are Independent from Each Other and Predictors

[TODO: check this!!!]

As a conclusion, usage of linear models and GLM is 
debatable for this task. However, for the log(Y) 
transformation, this model meets three of four assumptions. So, we will consider
this model and compare it with the rest.


# Full linear model

```{r}
full_model = lm(train[,Y_colname] ~ ., data = train[, X_colnames])
```

## Model summary
```{r}
summary(full_model)
```

```{r}
par(mfrow = c(2, 2))
plot(full_model)
par(mfrow = c(1, 1))
```

### Model residuals plot

```{r}
plot_residuals <- function(model){
  plot(
    fitted(model),
    residuals(model),
    col = 'gray40',
    xlab = 'fitted values',
    ylab = 'residuals'
  )
  
  lines(
    loess.smooth(fitted(model), residuals(model)),
    col = "blue",
    lwd = 2
  )
  abline(h = 0, lty = 3)
}

par(mfrow = c(1, 2))
plot(train$timedelta, train$shares)
abline(full_model, col = "blue", lwd = 2)
plot_residuals(full_model)
par(mfrow = c(1, 1))
```

```{r}
plot_residuals(full_model)
```


```{r}
plot(data$shares, data$n_tokens_content, pch = 5)
abline(full_model)
```




```{r}
# To look at coefficients
# coefficients(full_model)

# Confidence intervals for coefficients
# confint(full_model)

# Residuals
# e = residuals(full_model)

# summary(model)
# anova(full_model, reduced_model)  # Script #2 - F-test for comparison of models

# Update model
# red.mod <- update(full.mod, ~.-weight)
# summary(red.mod)
```

```{r}
plot(train$n_tokens_content, residuals(train))
plot(train$average_token_length, residuals(train))
```


# Full linear model (Y log transform) -- Poisson GLM

```{r}
full_model_log = lm(log(train[,Y_colname]) ~ ., data = train[, X_colnames])
```

## Model summary

```{r}
qqnorm(rstandard(full_model_log))
qqline(rstandard(full_model_log))
```


```{r}
summary(full_model_log)
```


```{r}
par(mfrow = c(2, 2))
plot(full_model_log)
par(mfrow = c(1, 1))
```

```{r}
plot_residuals(full_model_log)
```



# Linear model (forward / backward model selection)

Notice! That here we have two highly correlated features : 'weekday_is_friday' and 'weekday_is_saturday'


```{r}
correlated_indices = c(
  match('weekday_is_friday', names(train)),
  match('weekday_is_saturday', names(train))
)
# match
stopifnot(sum(is.na(correlated_indices)) == 0)


nvmax = 20

forward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

backward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

## Model summary

```{r}
plot_max_point <- function(values){
  max_idx = which.max(values)
  points(max_idx, values[max_idx], col = 'red', cex = 2, pch = 20)
}

plot_min_point <- function(values){
  min_idx = which.min(values)
  points(min_idx, values[min_idx], col = 'red', cex = 2, pch = 20)
}

plot_subsets_summary <- function(subsets_model){
  subset_summary = summary(subsets_model)
  
  xlabel = 'Number of iterations'
  linetype = 'l'
  
  par(mfrow = c(2, 2))
  
  plot(subset_summary$rss, xlab = xlabel, ylab = 'RSS', type = linetype)
  plot_min_point(values = subset_summary$rss)
  
  plot(subset_summary$adjr2, xlab = xlabel, ylab = 'Adjusted R2', type = linetype)
  plot_max_point(values = subset_summary$adjr2)
  
  plot(subset_summary$cp, xlab = xlabel, ylab = 'Cp', type = linetype)
  plot_min_point(values = subset_summary$cp)
  
  plot(subset_summary$bic, xlab = xlabel, ylab = 'BIC', type = 'l')
  plot_min_point(values = subset_summary$bic)
  
  
  # max_idx = which.max(subset_summary$adjr2)
  # points(max_idx, reg.summary$adjr2[max_idx], col="red",cex=2,pch=20)
  
  par(mfrow = c(1, 1))
}
```

```{r}
plot_subsets_summary(forward_subsets)
```

```{r}
plot_subsets_summary(backward_subsets)
```

## Resume of model selection (best number of parameters) and building these models

```{r}
get_best_n_params <- function(subsets_model){
  model_summary = summary(subsets_model)
  best_bic = which.min(model_summary$bic)
  best_cp = which.min(model_summary$cp)
  return (list('best_bic' = best_bic, 'best_cp' = best_cp))
}

get_best_coefficients <- function(subsets_model){
  best_params = get_best_n_params(subsets_model)
  
  n_best_bic = best_params$best_bic
  n_best_cp = best_params$best_cp
  
  best_bic_coefs = names(coefficients(subsets_model, n_best_bic))[-1]
  best_cp_coefs = names(coefficients(subsets_model, n_best_cp))[-1]
  
  return(list("best_bic_coefs" = best_bic_coefs, "best_cp_coefs" = best_cp_coefs))
}

best_params_back = get_best_n_params(backward_subsets)
best_params_forward = get_best_n_params(forward_subsets)

n_best_bic_back = best_params_back$best_bic
n_best_cp_back = best_params_back$best_cp

n_best_bic_fwd = best_params_forward$best_bic
n_best_cp_fwd = best_params_forward$best_cp

```


```{r}
cat('Best # of parameters (backward)', n_best_bic_back, n_best_cp_back, '\n')
cat('Best # of parameters (forward)', n_best_bic_fwd, n_best_cp_fwd, '\n')
```

```{r}
best_forward_coefs = get_best_coefficients(forward_subsets)
best_backward_coefs = get_best_coefficients(backward_subsets)

bic_forward_coefs = best_forward_coefs$best_bic_coefs
bic_backward_coefs = best_backward_coefs$best_bic_coefs

cp_forward_coefs = best_forward_coefs$best_cp_coefs
cp_backward_coefs = best_backward_coefs$best_cp_coefs
```

Here are the chosen coefficients after model selection:

```{r}
print(cp_backward_coefs)
print(bic_backward_coefs)
```


## The variables elimination 

```{r}
plot(backward_subsets, scale = 'r2')
```

```{r}
plot(forward_subsets, scale = 'r2')
```

Up to now, from lectures we know that backward model selection is more preferable than the forward. According to the backward procedure, we can highlight two models - with 19 parameters (minimum of CP criteria) and 7 parameters (Minimum of Bayesian Information Criteria). These models will be further tested

## Building best model after elimination (Based on BIC)

```{r}
bic_best_model = lm(train[, Y_colname] ~ ., data = train[, bic_backward_coefs])
```

```{r}
summary(bic_best_model)
```
## Building best model after elimination (Based on Cp)

```{r}
cp_best_model = lm(train[, Y_colname] ~ ., data = train[, cp_backward_coefs])
```

```{r}
summary(cp_best_model)
```

# Forward / backward selection for log(Y) transform
```{r}

nvmax = 20

log_forward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

log_backward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

```{r}
plot_subsets_summary(log_forward_subsets)
```

```{r}
plot_subsets_summary(log_backward_subsets)
```


```{r}
log_best_coefficients = get_best_coefficients(log_backward_subsets)
log_bic_backward_coefs = log_best_coefficients$best_bic_coefs
log_cp_backward_coefs = log_best_coefficients$best_cp_coefs
```


```{r}
cat(
  'Number of best coefficients', 
  length(log_bic_backward_coefs), 
  length(log_cp_backward_coefs),
  '\n'
)
log_bic_backward_coefs
log_cp_backward_coefs
```


```{r}
log_cp_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_cp_backward_coefs])
summary(log_cp_best_model)
```

```{r}
log_bic_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_bic_backward_coefs])
```


```{r}
summary(log_bic_best_model)
```
```{r}
preds = predict(
  log_bic_best_model,
  newdata = validation[, -length(colnames(validation))]
)

mean((exp(preds) - validation$shares)^2)

to = 100
plot(exp(preds[1:to]), ylim = c(0, 40))
lines(validation$shares[1:to], add = T)
```


# Ridge regression

```{r}
standardize_data <- function(df, target){
  vals = df[, target]
  stand_data = as.data.frame(scale(df))
  stand_data[, target] = vals
  return(stand_data)
}
```

```{r}
standardized_train = standardize_data(train, 'shares')
standardized_validation = standardize_data(validation, 'shares')
standardized_test = standardize_data(test, 'shares')
```

```{r}
prepare_ridge_lasso <- function(X, Y, alpha){
  grid <- 10^seq(10, -2, length=100)
  
  model <- glmnet(
    X, 
    Y,
    alpha = alpha,  # 0 for Ridge Regression
    lambda = grid,
    standardize = F
  )
  
  library(glmnet)

  set.seed(1)
  cv.out <- cv.glmnet(
    X,
    Y,
    alpha = alpha,
    nfold=10
  )
  
  plot(cv.out)
  
  i.bestlam <- which.min(cv.out$cvm)
  bestlam <- cv.out$lambda[i.bestlam]
  cat('Best lambda:', bestlam, '\n')
  
  return(list('model' = model, 'lambda' = bestlam))
}
```


```{r}
X = as.matrix(standardized_train[, -length(colnames(standardized_train))])
Y = as.matrix(standardized_train$shares)

ridge_res = prepare_ridge_lasso(
  X = X,
  Y = Y,
  alpha = 0  # Ridge
)

ridge_model = ridge_res$model
ridge_lamda = ridge_res$lambda
```


```{r}
ridge_predictions <- predict(
  ridge_model, 
  s = ridge_lamda,
  newx = as.matrix(
    standardized_validation[, -length(colnames(standardized_validation))]
  )
)

Ridge_MSE_val = mean((ridge.pred - standardized_validation$shares)^2)
```

```{r}
to = 100
plot(ridge.pred[1:to], ylim = c(0, 40))
lines(y.test[1:to])
```
# Lasso Regression

```{r}
lasso_res = prepare_ridge_lasso(
  X = X,
  Y = Y,
  alpha = 1  # Lasso
)

lasso_model = lasso_res$model
lasso_lambda = lasso_res$lambda
```
```{r}
lasso_predictions <- predict(
  lasso_model, 
  s = lasso_lambda,
  newx = as.matrix(
    standardized_validation[, -length(colnames(standardized_validation))]
  )
)

lasso_MSE_val = mean((lasso_predictions - standardized_validation$shares)^2)
```

# Comparison of models

How do we choose proper criteria for comparing models? We have several options:
1. R^2 (adjusted) on validation set
2. R^2 (adjusted) on train set
3. Bayesian Information Criteria
4. Akaike information criteria
5. Mallow's Cp criteria

We select the proper characteristics, according to following considerations:
1. Cp, AIC, BIC and adjusted R^2 - all these account for both good fit
and simplicity of the model
2. Validation R^2 parameter is preferred, as it shows the performance on 
the unseen data. Ideally, a cross-validation adjusted R^2 metrics will
give better understanding
3. Mallow's Cp and AIC are equivalent and result in the selection of
the same model
4. We are looking to minimize Cp, AIC and BIC, but to maximize R^2-related
metrics
5. BIC statistics penalizer big models heavier that Cp and AIC --> BIC criterion
results in selection of more 'lightweight' model

According to this, we will compare the models based on following characteristics:
1. (Cross) validation adjusted R^2
2. AIC (or, equivalently, Mallow's Cp)
3. BIC (BIC criterion will be of the highest priority, if we target at the
simplest model possible)


```{r}
models = list(
  full_model = list(name = 'full', model = full_model),
  full_model_log = list(name = 'full log', model = full_model_log),
  
  backward_bic_best_model = list(name = 'bck. bic', model = bic_best_model),
  backward_cp_best_model = list(name = 'bck. cp', model = cp_best_model),
  
  backward_cp_best_model_log = list(name = 'bck. cp log', 
                                    model = log_cp_best_model),
  backward_bic_best_model_log = list(name = 'bck. bic log',
                                     model = log_bic_best_model)
)
```

## Comparison of R^2

```{r}

plot_ordered_hist <- function(X, Y, main, xlab, ylab){
  data = data.frame(Y, X)
  barplot(
    data[order(data[,1],decreasing=TRUE),][,1],
    names.arg=data[order(data[,1],decreasing=TRUE),][,2],
    main = main,
    xlab = xlab,
    ylab = ylab
  )
}

names = c()
r.squareds = c()
for(m in models){
  names = append(names, m$name)  
  r.squareds = append(r.squareds, summary(m$model)$adj.r.squared)
}

plot_ordered_hist(
  X = names,
  Y = r.squareds,
  main = 'Adjusted R sqared',
  xlab = 'Model name',
  ylab = 'Adjusted R squared'
)
```
## Comparison of AIC

```{r}
library(olsrr)

AICs = c()

for(m in models){
  AICs = append(AICs, AIC(m$model))
}

AICs_norm = AICs / max(AICs)
plot_ordered_hist(
  X = names, 
  Y = AICs_norm, 
  main = 'Normalized Akaike Information Criterion (AIC)', 
  xlab = 'Model name', 
  ylab = 'AIC'
)
```
## Comparison of BIC

```{r}
BICs = c()

for(m in models){
  BICs = append(BICs, BIC(m$model))
}

BICs_norm = BICs / max(BICs)
plot_ordered_hist(
  X = names, 
  Y = BICs_norm, 
  main = 'Normalized Bayesian Information Criterion (BIC)', 
  xlab = 'Model name', 
  ylab = 'BIC'
)
```

## Comparison of parameters number

```{r}
model_sizes = c()
for(m in models){
  model_sizes = append(model_sizes, length(coefficients(m$model)))
}
model_sizes_norm = model_sizes / max(model_sizes)

plot_ordered_hist(
  X = names, 
  Y = model_sizes_norm, 
  main = 'Number of coefficients', 
  xlab = 'Model name', 
  ylab = 'Normalized number of coefficients'
)
```

## Comparison on the validation set

```{r}
library(stringr)

validation_r2 = c()

for(m in models){
  model = m$model
  name = m$name

  preds = predict(
    model,
    data = validation[, coefficients(model)]
  )
  
  if(str_detect(name, 'log')){
    preds = exp(preds)
  }
  
  r2 = mean((preds - test$shares)^2)
  cat('Model:', name, 'MSE validation:', r2, '\n')
  validation_r2 = append(validation_r2, r2)
}


names_ = append(names, c('Ridge', 'Lasso'))
validation_r2_ = append(validation_r2, c(1.0, 1.0))

plot_ordered_hist(
  X = append(names, c('Ridge', 'Lasso')), #_names,
  Y = append(validation_r2, c(Ridge_MSE_val, lasso_MSE_val)), #_validation_r2,
  main = 'MSE validation set', 
  xlab = 'Model name', 
  ylab = 'MSE'
)
```






