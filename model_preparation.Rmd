---
title: "Data processing"
output: html_notebook
---

Ctrl + Shift + Enter - run cell.
Ctrl + Alt + I - insert new cell.
Ctrl + Shift + K - preview HTML result document.

# Packages
```{r}
library(leaps)
library('randomForest')
library('e1071')
library(class)
library(tidyverse)
library(scales)
library(caret)
library(ggplot2)

```


# Download data

```{r}
file_path = './Data/OnlineNewsPopularity_processed.csv'
data = read.csv(file_path)
print(dim(data))
head(data)
```


```{r}
data = data[, -1] # removing url column
head(data)
stopifnot(dim(data)[2] == 60)
```


# Subsetting training data

```{r}
subset_size = 3000  # Number of points that we use from the dataset

subsample_indices = sample.int(
  n = nrow(data),
  size = subset_size
)

data = data[subsample_indices, ]
stopifnot(dim(data)[1] == subset_size)
```

```{r}
train_percent = 0.6
val_percent = 0.2
train_val_percent = train_percent + val_percent
test_percent = 1 - train_val_percent
```

```{r}
train_val_sample = sample.int(
  n = nrow(data),
  size = floor(train_val_percent * nrow(data))
)

train_val = data[train_val_sample, ]
test = data[-train_val_sample, ]

train_sample = sample.int(
  n = nrow(train_val),
  size = floor(train_percent * nrow(data)) 
)

train = train_val[train_sample, ]
validation = train_val[-train_sample, ]

ncat = function(...){
  cat(..., '\n')
}

ncat('Train size', nrow(train))
ncat('Validation size', nrow(validation))
ncat('Test size', nrow(test))
```

```{r}
Y_colname = colnames(data)[length(colnames(data))]
stopifnot(Y_colname == 'shares')

X_colnames = colnames(data)[1 : length(colnames(data)) - 1]

# X = as.matrix(data[, X_colnames])
# Y = data[, Y_colname]

stopifnot(length(Y_colname) + length(X_colnames) == dim(data)[2])
stopifnot(dim(train)[2] - 1 == dim(train[, X_colnames])[2])
```


# Full linear model

```{r}
full_model = lm(train[,Y_colname] ~ ., data = train[, X_colnames])
```

## Model summary
```{r}
summary(full_model)
```

```{r}
par(mfrow = c(2, 2))
plot(full_model)
par(mfrow = c(1, 1))
```

```{r}
qqnorm(rstandard(full_model))
qqline(rstandard(full_model))
```

### Model residuals plot

```{r}
colnames(train)
```
```{r}
plot_residuals <- function(model){
  plot(
    fitted(model),
    residuals(model),
    col = 'gray40',
    xlab = 'fitted values',
    ylab = 'residuals'
  )
  
  lines(
    loess.smooth(fitted(model), residuals(model)),
    col = "blue",
    lwd = 2
  )
  abline(h = 0, lty = 3)
}

par(mfrow = c(1, 2))
plot(train$timedelta, train$shares)
abline(full_model, col = "blue", lwd = 2)
plot_residuals(full_model)
par(mfrow = c(1, 1))
```

```{r}
plot_residuals(full_model)
```


```{r}
plot(data$shares, data$n_tokens_content, pch = 5)
abline(full_model)
```




```{r}
# To look at coefficients
# coefficients(full_model)

# Confidence intervals for coefficients
# confint(full_model)

# Residuals
# e = residuals(full_model)

# summary(model)
# anova(full_model, reduced_model)  # Script #2 - F-test for comparison of models

# Update model
# red.mod <- update(full.mod, ~.-weight)
# summary(red.mod)
```

```{r}
e <- residuals(full_model)
RSS <- sum(e^2)
RSS
```

```{r}
plot(train$n_tokens_content, residuals(train))
plot(train$average_token_length, residuals(train))
```







# Full linear model (Y log transform) -- Poisson GLM

```{r}
full_model_log = lm(log(train[,Y_colname]) ~ ., data = train[, X_colnames])
```

## Model summary

```{r}
qqnorm(rstandard(full_model_log))
qqline(rstandard(full_model_log))
```


```{r}
summary(full_model_log)
```


```{r}
par(mfrow = c(2, 2))
plot(full_model_log)
par(mfrow = c(1, 1))
```

```{r}
plot_residuals(full_model_log)
```



# Linear model (forward / backward model selection)

Notice! That here we have two highly correlated features : 'weekday_is_friday' and 'weekday_is_saturday'


```{r}
correlated_indices = c(
  match('weekday_is_friday', names(train)),
  match('weekday_is_saturday', names(train))
)
# match
stopifnot(sum(is.na(correlated_indices)) == 0)


nvmax = 20

forward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

backward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

## Model summary

```{r}
plot_max_point <- function(values){
  max_idx = which.max(values)
  points(max_idx, values[max_idx], col = 'red', cex = 2, pch = 20)
}

plot_min_point <- function(values){
  min_idx = which.min(values)
  points(min_idx, values[min_idx], col = 'red', cex = 2, pch = 20)
}

plot_subsets_summary <- function(subsets_model){
  subset_summary = summary(subsets_model)
  
  xlabel = 'Number of iterations'
  linetype = 'l'
  
  par(mfrow = c(2, 2))
  
  plot(subset_summary$rss, xlab = xlabel, ylab = 'RSS', type = linetype)
  plot_min_point(values = subset_summary$rss)
  
  plot(subset_summary$adjr2, xlab = xlabel, ylab = 'Adjusted R2', type = linetype)
  plot_max_point(values = subset_summary$adjr2)
  
  plot(subset_summary$cp, xlab = xlabel, ylab = 'Cp', type = linetype)
  plot_min_point(values = subset_summary$cp)
  
  plot(subset_summary$bic, xlab = xlabel, ylab = 'BIC', type = 'l')
  plot_min_point(values = subset_summary$bic)
  
  
  # max_idx = which.max(subset_summary$adjr2)
  # points(max_idx, reg.summary$adjr2[max_idx], col="red",cex=2,pch=20)
  
  par(mfrow = c(1, 1))
}
```

## Model summary

```{r}
plot_subsets_summary(forward_subsets)
```

```{r}
plot_subsets_summary(backward_subsets)
```
## Resume of model selection (best number of parameters) and building these models

```{r}
get_best_n_params <- function(subsets_model){
  model_summary = summary(subsets_model)
  best_bic = which.min(model_summary$bic)
  best_cp = which.min(model_summary$cp)
  return (list('best_bic' = best_bic, 'best_cp' = best_cp))
}

get_best_coefficients <- function(subsets_model){
  best_params = get_best_n_params(subsets_model)
  
  n_best_bic = best_params$best_bic
  n_best_cp = best_params$best_cp
  
  best_bic_coefs = names(coefficients(subsets_model, n_best_bic))[-1]
  best_cp_coefs = names(coefficients(subsets_model, n_best_cp))[-1]
  
  return(list("best_bic_coefs" = best_bic_coefs, "best_cp_coefs" = best_cp_coefs))
}

best_params_back = get_best_n_params(backward_subsets)
best_params_forward = get_best_n_params(forward_subsets)

n_best_bic_back = best_params_back$best_bic
n_best_cp_back = best_params_back$best_cp

n_best_bic_fwd = best_params_forward$best_bic
n_best_cp_fwd = best_params_forward$best_cp

```


```{r}
cat('Best # of parameters (backward)', n_best_bic_back, n_best_cp_back, '\n')
cat('Best # of parameters (forward)', n_best_bic_fwd, n_best_cp_fwd, '\n')
```
```{r}
best_forward_coefs = get_best_coefficients(forward_subsets)
best_backward_coefs = get_best_coefficients(backward_subsets)

bic_forward_coefs = best_forward_coefs$best_bic_coefs
bic_backward_coefs = best_backward_coefs$best_bic_coefs

cp_forward_coefs = best_forward_coefs$best_cp_coefs
cp_backward_coefs = best_backward_coefs$best_cp_coefs
```

Here are the chosen coefficients after model selection:

```{r}
print(cp_backward_coefs)
print(bic_backward_coefs)
```


## The variables elimination 

```{r}
plot(backward_subsets, scale = 'r2')
```

```{r}
plot(forward_subsets, scale = 'r2')
```

Up to now, from lectures we know that backward model selection is more preferable than the forward. According to the backward procedure, we can highlight two models - with 19 parameters (minimum of CP criteria) and 7 parameters (Minimum of Bayesian Information Criteria). These models will be further tested


## Building best model after elimination (Based on BIC)

```{r}
bic_best_model = lm(train[, Y_colname] ~ ., data = train[, bic_backward_coefs])
```

```{r}
summary(bic_best_model)
```
## Building best model after elimination (Based on Cp)

```{r}
cp_best_model = lm(train[, Y_colname] ~ ., data = train[, cp_backward_coefs])
```

```{r}
summary(cp_best_model)
```


# Forward / backward selection for log(Y) transform
```{r}
nvmax = 20

log_forward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

log_backward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

```{r}
plot_subsets_summary(log_forward_subsets)
```
```{r}
plot_subsets_summary(log_backward_subsets)
```
```{r}
log_best_coefficients = get_best_coefficients(log_backward_subsets)
log_bic_backward_coefs = log_best_coefficients$best_bic_coefs
log_cp_backward_coefs = log_best_coefficients$best_cp_coefs
```


```{r}
cat(
  'Number of best coefficients', 
  length(log_bic_backward_coefs), 
  length(log_cp_backward_coefs),
  '\n'
)
log_bic_backward_coefs
log_cp_backward_coefs
```


```{r}
log_cp_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_cp_backward_coefs])
summary(log_cp_best_model)
```

```{r}
log_bic_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_bic_backward_coefs])
```


```{r}
summary(log_bic_best_model)
```

# Comparison of models

```{r}
models = list(
  full_model = list(name = 'full', model = full_model),
  full_model_log = list(name = 'full log', model = full_model_log),
  
  backward_bic_best_model = list(name = 'bck. bic', model = bic_best_model),
  backward_cp_best_model = list(name = 'bck. cp', model = cp_best_model),
  
  backward_cp_best_model_log = list(name = 'bck. cp log', 
                                    model = log_cp_best_model),
  backward_bic_best_model_log = list(name = 'bck. bic log',
                                     model = log_bic_best_model)
)
```

## Comparison of R^2

```{r}

plot_ordered_hist <- function(X, Y, main, xlab, ylab){
  data = data.frame(Y, X)
  barplot(
    data[order(data[,1],decreasing=TRUE),][,1],
    names.arg=data[order(data[,1],decreasing=TRUE),][,2],
    main = main,
    xlab = xlab,
    ylab = ylab
  )
}

names = c()
r.squareds = c()
for(m in models){
  names = append(names, m$name)  
  r.squareds = append(r.squareds, summary(m$model)$adj.r.squared)
}

plot_ordered_hist(
  X = names,
  Y = r.squareds,
  main = 'Adjusted R sqared',
  xlab = 'Model name',
  ylab = 'Adjusted R squared'
)
```


## Comparison of BIC

```{r}
BICs = c()

for(m in models){
  BICs = append(BICs, BIC(m$model))
}

BICs_norm = BICs / max(BICs)
plot_ordered_hist(
  X = names, 
  Y = BICs_norm, 
  main = 'Normalized Bayesian Information Criterion (BIC)', 
  xlab = 'Model name', 
  ylab = 'BIC'
)
```

## Comparison of parameters number

```{r}
model_sizes = c()
for(m in models){
  model_sizes = append(model_sizes, length(coefficients(m$model)))
}
model_sizes_norm = model_sizes / max(model_sizes)

plot_ordered_hist(
  X = names, 
  Y = model_sizes_norm, 
  main = 'Number of coefficients', 
  xlab = 'Model name', 
  ylab = 'Normalized number of coefficients'
)
```

## Comparison on the test set

```{r}
library(stringr)

test_r2 = c()

for(m in models){
  model = m$model
  name = m$name

  preds = predict(
    model,
    data = test[, coefficients(model)]
  )
  if(str_detect(name, 'log')){
    preds = exp(preds)
  }
  
  r2 = mean((preds - test$shares)^2)
  cat('Model:', name, 'mean test R2:', r2, '\n')
  test_r2 = append(test_r2, r2)
}

plot_ordered_hist(
  X = names, 
  Y = test_r2,
  main = 'R2 test', 
  xlab = 'Model name', 
  ylab = 'R2'
)
```

#Random Forest 

```{r}
set.seed(123)
model1 <- randomForest(train_val [,Y_colname] ~ .,data = train_val)
model1
plot(model1)

```

```{r}
# number of trees with lowest MSE- lowest error rate
which.min(model1$mse) 

# RMSE of this optimal random forest
sqrt(model1$mse[which.min(model1$mse)]) 
```

```{r}
##Rf- predicting accuracy using validation set
# validation data

x_test <- validation[setdiff(names(validation), "Y_colname")] ##all predictors of the valid set
y_test <- validation$Y_colname

rf_oob_comp <- randomForest(train[,Y_colname] ~ .,data = train, xtest = x_test,ytest = y_test)
```

#Extract OOB & Validation errors
```{r}
oob <- sqrt(rf_oob_comp$mse)
validation_error <- sqrt(rf_oob_comp$test$mse)
```

# Compare error rates
```{r}
tibble::tibble(
  'Out of Bag Error' = oob,
  'Test error' = validation_error,
  ntrees = 1:rf_oob_comp$ntree) %>% gather(Metric, RMSE, -ntrees) %>% ggplot(aes(ntrees, RMSE, color = Metric)) + geom_line() + scale_y_continuous(labels = comma) + xlab("Number of trees")

```


```{r}
##Tunning the model
##tune the mtry parameter starting from mtry=5 to obtain optimal value of OOB error
# names of features of the initial train set
features <- setdiff(names(train), "Y_colname")
set.seed(123)

##start with mtry = 5 and increases by a factor of 1.5 until the OOB error stops improving by 1%. 

model2 <- tuneRF(
  x = train[features],
  y = train$Y_colname,
  ntreeTry   = 500,
  mtryStart  = 5,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE ) 
model2
```

 

#KNN 

```{r}
set.seed(123)
dat.d <- sample(1:nrow(data),size=nrow(data)*0.7,replace = FALSE) #random selection of 70% data.
 
train.data <- data[dat.d,] # 70% training data
test.data <- data[-dat.d,] # remaining 30% test data

```

```{r}
#Creating seperate dataframe for 'shares' feature which is our target.
train.share <- data[dat.d,1]
test.share <-data[-dat.d,1]

```

```{r}
str(data)
```

```{r}
NROW(train.share) 
```
o, we have 13978 observations in our training data set. The square root of 13978 is around 118.23, therefore we’ll create two models. One with ‘K’ value as 118 and the other model with a ‘K’ value as 119.

```{r}
knn.118 <- knn(train=train.data, test=test.data, cl=train.share, k=118)
knn.119 <- knn(train=train.data, test=test.data, cl=train.share, k=119)

```


#Compute the accuracy 
```{r}
#Calculate the proportion of correct classification for k = 118, 119
ACC.118 <- 100 * sum(test.share == knn.118)/NROW(test.share)
ACC.119 <- 100 * sum(test.share == knn.119)/NROW(test.share)
 
ACC.118

ACC.119

```


```{r}
# Check prediction against actual value in tabular form for k=118
table(knn.118 ,test.share)
```

```{r}
# Check prediction against actual value in tabular form for k=119
table(knn.119 ,test.share)
```


We can also use the confusion matrix to calculate the accuracy of the KNN model: 
```{r}
confusionMatrix(table(knn.118 ,test.share))
```

#Optimization  

#SVM
```{r}
set.seed(123)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm_Linear <- train(shares ~., data = train, method = "svmLinear",
                    trControl=trctrl,
                    preProcess = c("center", "scale"),
                    tuneLength = 10)
svm_pred <- predict(svm_Linear, data = test)

plot(svm_Linear_Grid)
confusionMatrix(svm_pred, test$shares )
```



