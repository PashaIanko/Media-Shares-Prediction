---
title: "Data processing"
output: html_notebook
---

Ctrl + Shift + Enter - run cell.
Ctrl + Alt + I - insert new cell.
Ctrl + Shift + K - preview HTML result document.

# Packages
```{r}
library(leaps)
```


# Download data

```{r}
file_path = './Data/OnlineNewsPopularity_processed.csv'
data = read.csv(file_path)
print(dim(data))
head(data)
```

```{r}
data = data[, -1] # removing url column
head(data)
stopifnot(dim(data)[2] == 60)
```


# Subsetting training data

```{r}
train_percent = 0.1  # percent of initial dataset
val_percent = 0.1
train_val_percent = train_percent + val_percent
test_percent = 1 - train_val_percent
```

```{r}
train_val_sample = sample.int(
  n = nrow(data),
  size = floor(train_val_percent * nrow(data))
)

train_val = data[train_val_sample, ]
test = data[-train_val_sample, ]

train_sample = sample.int(
  n = nrow(train_val),
  size = floor(train_percent * nrow(data)) 
)

train = train_val[train_sample, ]
validation = train_val[-train_sample, ]

ncat = function(...){
  cat(..., '\n')
}

ncat('Train size', nrow(train))
ncat('Validation size', nrow(validation))
ncat('Test size', nrow(test))
```



```{r}
Y_colname = colnames(data)[length(colnames(data))]
stopifnot(Y_colname == 'shares')

X_colnames = colnames(data)[1 : length(colnames(data)) - 1]

# X = as.matrix(data[, X_colnames])
# Y = data[, Y_colname]

stopifnot(length(Y_colname) + length(X_colnames) == dim(data)[2])
stopifnot(dim(train)[2] - 1 == dim(train[, X_colnames])[2])
```


# Full linear model

```{r}
full_model = lm(train[,Y_colname] ~ ., data = train[, X_colnames])
```


## Model summary
```{r}
summary(full_model)
```

```{r}
par(mfrow = c(2, 2))
plot(full_model)
par(mfrow = c(1, 1))
```

```{r}
qqnorm(rstandard(full_model))
qqline(rstandard(full_model))
```

### Model residuals plot

```{r}
colnames(train)
```
```{r}
plot_residuals <- function(model){
  plot(
    fitted(model),
    residuals(model),
    col = 'gray40',
    xlab = 'fitted values',
    ylab = 'residuals'
  )
  
  lines(
    loess.smooth(fitted(model), residuals(model)),
    col = "blue",
    lwd = 2
  )
  abline(h = 0, lty = 3)
}

par(mfrow = c(1, 2))
plot(train$timedelta, train$shares)
abline(full_model, col = "blue", lwd = 2)
plot_residuals(full_model)
par(mfrow = c(1, 1))
```

```{r}
plot_residuals(full_model)
```


```{r}
plot(data$shares, data$n_tokens_content, pch = 5)
abline(full_model)
```




```{r}
# To look at coefficients
# coefficients(full_model)

# Confidence intervals for coefficients
# confint(full_model)

# Residuals
# e = residuals(full_model)

# summary(model)
# anova(full_model, reduced_model)  # Script #2 - F-test for comparison of models

# Update model
# red.mod <- update(full.mod, ~.-weight)
# summary(red.mod)
```

```{r}
e <- residuals(full_model)
RSS <- sum(e^2)
RSS
```

```{r}
plot(train$n_tokens_content, residuals(train))
plot(train$average_token_length, residuals(train))
```







# Full linear model (Y log transform) -- Poisson GLM

```{r}
full_model_log = lm(log(train[,Y_colname]) ~ ., data = train[, X_colnames])
```

## Model summary

```{r}
qqnorm(rstandard(full_model_log))
qqline(rstandard(full_model_log))
```


```{r}
summary(full_model_log)
```


```{r}
par(mfrow = c(2, 2))
plot(full_model_log)
par(mfrow = c(1, 1))
```

```{r}
plot_residuals(full_model_log)
```



# Linear model (forward / backward model selection)

Notice! That here we have two highly correlated features : 'weekday_is_friday' and 'weekday_is_saturday'


```{r}
correlated_indices = c(
  match('weekday_is_friday', names(train)),
  match('weekday_is_saturday', names(train))
)
# match
stopifnot(sum(is.na(correlated_indices)) == 0)


nvmax = 20

forward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

backward_subsets = regsubsets(
  shares ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

## Model summary

```{r}
plot_max_point <- function(values){
  max_idx = which.max(values)
  points(max_idx, values[max_idx], col = 'red', cex = 2, pch = 20)
}

plot_min_point <- function(values){
  min_idx = which.min(values)
  points(min_idx, values[min_idx], col = 'red', cex = 2, pch = 20)
}

plot_subsets_summary <- function(subsets_model){
  subset_summary = summary(subsets_model)
  
  xlabel = 'Number of iterations'
  linetype = 'l'
  
  par(mfrow = c(2, 2))
  
  plot(subset_summary$rss, xlab = xlabel, ylab = 'RSS', type = linetype)
  plot_min_point(values = subset_summary$rss)
  
  plot(subset_summary$adjr2, xlab = xlabel, ylab = 'Adjusted R2', type = linetype)
  plot_max_point(values = subset_summary$adjr2)
  
  plot(subset_summary$cp, xlab = xlabel, ylab = 'Cp', type = linetype)
  plot_min_point(values = subset_summary$cp)
  
  plot(subset_summary$bic, xlab = xlabel, ylab = 'BIC', type = 'l')
  plot_min_point(values = subset_summary$bic)
  
  
  # max_idx = which.max(subset_summary$adjr2)
  # points(max_idx, reg.summary$adjr2[max_idx], col="red",cex=2,pch=20)
  
  par(mfrow = c(1, 1))
}
```

## Model summary

```{r}
plot_subsets_summary(forward_subsets)
```

```{r}
plot_subsets_summary(backward_subsets)
```
## Resume of model selection (best number of parameters) and building these models

```{r}
get_best_n_params <- function(subsets_model){
  model_summary = summary(subsets_model)
  best_bic = which.min(model_summary$bic)
  best_cp = which.min(model_summary$cp)
  return (list('best_bic' = best_bic, 'best_cp' = best_cp))
}

get_best_coefficients <- function(subsets_model){
  best_params = get_best_n_params(subsets_model)
  
  n_best_bic = best_params$best_bic
  n_best_cp = best_params$best_cp
  
  best_bic_coefs = names(coefficients(subsets_model, n_best_bic))[-1]
  best_cp_coefs = names(coefficients(subsets_model, n_best_cp))[-1]
  
  return(list("best_bic_coefs" = best_bic_coefs, "best_cp_coefs" = best_cp_coefs))
}

best_params_back = get_best_n_params(backward_subsets)
best_params_forward = get_best_n_params(forward_subsets)

n_best_bic_back = best_params_back$best_bic
n_best_cp_back = best_params_back$best_cp

n_best_bic_fwd = best_params_forward$best_bic
n_best_cp_fwd = best_params_forward$best_cp

```


```{r}
cat('Best # of parameters (backward)', n_best_bic_back, n_best_cp_back, '\n')
cat('Best # of parameters (forward)', n_best_bic_fwd, n_best_cp_fwd, '\n')
```
```{r}
best_forward_coefs = get_best_coefficients(forward_subsets)
best_backward_coefs = get_best_coefficients(backward_subsets)

bic_forward_coefs = best_forward_coefs$best_bic_coefs
bic_backward_coefs = best_backward_coefs$best_bic_coefs

cp_forward_coefs = best_forward_coefs$best_cp_coefs
cp_backward_coefs = best_backward_coefs$best_cp_coefs
```

Here are the chosen coefficients after model selection:

```{r}
print(cp_backward_coefs)
print(bic_backward_coefs)
```


## The variables elimination 

```{r}
plot(backward_subsets, scale = 'r2')
```

```{r}
plot(forward_subsets, scale = 'r2')
```

Up to now, from lectures we know that backward model selection is more preferable than the forward. According to the backward procedure, we can highlight two models - with 19 parameters (minimum of CP criteria) and 7 parameters (Minimum of Bayesian Information Criteria). These models will be further tested


## Building best model after elimination (Based on BIC)

```{r}
bic_best_model = lm(train[, Y_colname] ~ ., data = train[, bic_backward_coefs])
```

```{r}
summary(bic_best_model)
```
## Building best model after elimination (Based on Cp)

```{r}
cp_best_model = lm(train[, Y_colname] ~ ., data = train[, cp_backward_coefs])
```

```{r}
summary(cp_best_model)
```


# Forward / backward selection for log(Y) transform
```{r}
nvmax = 20

log_forward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'forward',
  nvmax = nvmax
)

log_backward_subsets = regsubsets(
  log(shares) ~ .,
  data = train[, -correlated_indices], 
  method = 'backward',
  nvmax = 20
)
```

```{r}
plot_subsets_summary(log_forward_subsets)
```
```{r}
plot_subsets_summary(log_backward_subsets)
```
```{r}
log_best_coefficients = get_best_coefficients(log_backward_subsets)
log_bic_backward_coefs = log_best_coefficients$best_bic_coefs
log_cp_backward_coefs = log_best_coefficients$best_cp_coefs
```



```{r}
cat(
  'Number of best coefficients', 
  length(log_bic_backward_coefs), 
  length(log_cp_backward_coefs),
  '\n'
)
log_bic_backward_coefs
log_cp_backward_coefs
```


```{r}
log_cp_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_cp_backward_coefs])
```

```{r}
summary(log_cp_best_model)
```
```{r}
log_bic_best_model = lm(log(train[, Y_colname]) ~ ., data = train[, log_bic_backward_coefs])
```


```{r}
summary(log_bic_best_model)
```

# Comparison of models

```{r}
models = list(
  full_model = list(name = 'full', model = full_model),
  full_model_log = list(name = 'full log', model = full_model_log),
  
  backward_bic_best_model = list(name = 'bck. bic', model = bic_best_model),
  backward_cp_best_model = list(name = 'bck. cp', model = cp_best_model),
  
  backward_cp_best_model_log = list(name = 'bck. cp log', 
                                    model = log_cp_best_model),
  backward_bic_best_model_log = list(name = 'bck. bic log',
                                     model = log_bic_best_model)
)
```

## Comparison of R^2

```{r}

plot_ordered_hist <- function(X, Y, main, xlab, ylab){
  data = data.frame(Y, X)
  barplot(
    data[order(data[,1],decreasing=TRUE),][,1],
    names.arg=data[order(data[,1],decreasing=TRUE),][,2],
    main = main,
    xlab = xlab,
    ylab = ylab
  )
}

names = c()
r.squareds = c()
for(m in models){
  names = append(names, m$name)  
  r.squareds = append(r.squareds, summary(m$model)$adj.r.squared)
}

plot_ordered_hist(
  X = names,
  Y = r.squareds,
  main = 'Adjusted R sqared',
  xlab = 'Model name',
  ylab = 'Adjusted R squared'
)
```


## Comparison of BIC

```{r}
BICs = c()

for(m in models){
  BICs = append(BICs, BIC(m$model))
}

BICs_norm = BICs / max(BICs)
plot_ordered_hist(
  X = names, 
  Y = BICs_norm, 
  main = 'Normalized Bayesian Information Criterion (BIC)', 
  xlab = 'Model name', 
  ylab = 'BIC'
)
```

## Comparison of parameters number

```{r}
model_sizes = c()
for(m in models){
  model_sizes = append(model_sizes, length(coefficients(m$model)))
}
model_sizes_norm = model_sizes / max(model_sizes)

plot_ordered_hist(
  X = names, 
  Y = model_sizes_norm, 
  main = 'Number of coefficients', 
  xlab = 'Model name', 
  ylab = 'Normalized number of coefficients'
)
```

## Comparison on the test set

```{r}

```


